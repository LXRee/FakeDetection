{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CRNN\n",
    "class Dataset_CRNN(data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "    def __init__(self, data_path, frame_length=10, transform=None):\n",
    "        \"Initialization\"\n",
    "        self.transform = transform\n",
    "        #self.frames = frames\n",
    "        self.folders = data_path\n",
    "        self.frames = frame_length #For our case since we are computing 10 frames always\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(os.listdir(self.folders))\n",
    "\n",
    "    def read_images(self, data_path, use_transform):\n",
    "        X = []\n",
    "        file_name = \"\"\n",
    "        for i in os.listdir(data_path):\n",
    "            file_name = i\n",
    "            image = Image.open(os.path.join(data_path,i))\n",
    "            \n",
    "            #print(image.shape)\n",
    "            if use_transform is not None:\n",
    "                image = use_transform(image)\n",
    "                #print(image.size)\n",
    "            image = torch.from_numpy(np.asarray(image))\n",
    "            X.append(image)\n",
    "        X = torch.stack(X, dim=0)\n",
    "\n",
    "        return X, file_name\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = os.path.join(self.folders,os.listdir(self.folders)[index])\n",
    "              \n",
    "        # Load data\n",
    "        X, file_name = self.read_images(data_path, self.transform)                     # (input) spatial images\n",
    "        \n",
    "        y = np.ones(self.frames)\n",
    "        if 'real' in data_path:\n",
    "            y = np.zeros(self.frames)\n",
    "        #print(\"Folder is {}\".format(data_path))\n",
    "        #print(X.shape)\n",
    "        return X, torch.from_numpy(y).type(torch.LongTensor), file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_IMG = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.CenterCrop(256),\n",
    "    #transforms.ToTensor()\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         #std=[0.229, 0.224, 0.225] )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/home/chinmay/datatset/deepfake_split/train'\n",
    "train_data = Dataset_CRNN(train_path, transform=TRANSFORM_IMG, frame_length=30 )\n",
    "# for step, (x, y) in enumerate(data):\n",
    "#     print(x.shape)\n",
    "val_path = '/home/chinmay/datatset/deepfake_split/val'\n",
    "val_data = Dataset_CRNN(val_path, transform=TRANSFORM_IMG, frame_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 40\n",
    "log_interval = 20\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Meso4 import Meso4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Meso4(\n",
       "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batch_norm_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (conv2): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (batch_norm_2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (batch_norm_3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (batch_norm_4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (max_pool_2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (max_pool_4): MaxPool2d(kernel_size=(4, 4), stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc_conv): Conv2d(16, 2, kernel_size=(16, 16), stride=(1, 1))\n",
       "  (dropout): Dropout2d(p=0.2)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\") \n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = data.DataLoader(train_data, **params)\n",
    "valid_loader = data.DataLoader(val_data, **params)\n",
    "#model = Meso4()\n",
    "#model.cuda()\n",
    "# for index, (X,y) in enumerate(train_loader):\n",
    "#     y_pred = model(X)\n",
    "#     print (y_pred.shape)\n",
    "# Shape was torch.Size([4, 10, 2])\n",
    "model = Meso4()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_median(numpy_array = []): #This is expected to take an array of array. So,\n",
    "    #print(\"Input array is {}\".format(numpy_array))\n",
    "    output = []\n",
    "    confidence_scores = []\n",
    "    for array in numpy_array:\n",
    "        counts = np.bincount(array)\n",
    "        output.append(np.argmax(counts))\n",
    "        # Let us compute the confidence of the scores\n",
    "        # since frames are independent, our confidence is purely based on the number\n",
    "        # of frames our model thinks is belonging to a specific category\n",
    "        # the confidence of individual frame prediction is not taken into consideration\n",
    "        # and this portion is debatable....\n",
    "        frame_set_pred = np.sort(counts)[-1]\n",
    "        confidence = frame_set_pred/sum(counts)\n",
    "        confidence_scores.append(confidence)\n",
    "    return torch.from_numpy(np.asarray(output)).type(torch.LongTensor), torch.from_numpy(np.asarray(confidence_scores)).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_train = SummaryWriter('/home/chinmay/training-results/mesonet_mit_scores/train')\n",
    "writer_test = SummaryWriter('/home/chinmay/training-results/mesonet_mit_scores/test')\n",
    "save_model_path = \"/home/chinmay/model_weights/MesoNet-df/Mesonet_mit_scores\"\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train() #Put the model in training mode\n",
    "    losses = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    scores = []\n",
    "    #single_iter_loss = []\n",
    "    for batch_idx, (X, y, file_name) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)   # output has dim = (batch, number of classes)\n",
    "        #print(output)\n",
    "        #print(y.shape)\n",
    "        loss_train = 0\n",
    "        for items in range(output.shape[1]):\n",
    "            loss = loss_function(output[:,items,:], y[:,items])\n",
    "            loss_train = loss_train + loss.item()\n",
    "            #single_iter_loss.append(loss)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "        #print(\"backward update\")\n",
    "        #loss_batch = np.sum(single_iter_loss)\n",
    "        #loss_batch.backward(retain_graph=True) #Possible improvement to increase speed\n",
    "        #optimizer.step()\n",
    "        \n",
    "        losses.append(loss_train)\n",
    "\n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(output, 2)[1]  # y_pred != output score note used here\n",
    "        # We take the modal value of all predictions. So,\n",
    "        y, _ = find_median(y) #score param is meaningless\n",
    "        y_pred, _ = find_median(y_pred)\n",
    "        #print(\"filename is {}\".format(file_name))\n",
    "        #print(\"prediction is {}\".format(y_pred))\n",
    "        \n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        scores.append(step_score)         # computed on CPU\n",
    "        #print(step_score)\n",
    "        \n",
    "\n",
    "        # show information\n",
    "                 \n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss_train, 100 * step_score))\n",
    "        \n",
    "    # we should sum all these losses and then divide by the training batch size\n",
    "    losses_scalar = np.sum(losses)/ len(train_loader.dataset) \n",
    "    return losses_scalar, np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X, y,file_name in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            #print(\"Output shape is {}\".format(output.shape))\n",
    "            #print(\"Expected shape is {}\".format(y.shape))\n",
    "            for items in range(output.shape[1]):\n",
    "                loss = loss_function(output[:,items,:], y[:,items])\n",
    "                test_loss += loss.item()                 # sum up batch loss\n",
    "            #Predict value\n",
    "            y_pred = torch.max(output, 2)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "            # Replace the value with the median value\n",
    "            y_pred, score = find_median(y_pred)\n",
    "            y, _ = find_median(y) #score param is meaningless\n",
    "            \n",
    "            # print few results\n",
    "            print(\"Prediction is {} and score is {}\".format(y_pred,score))\n",
    "            \n",
    "            \n",
    "            # print(\"actual value of labels read from disc is {}\".format(y))\n",
    "            # print(\"filename is {} and prediction is {}\".format(file_name, y_pred))\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(model.state_dict(), os.path.join(save_model_path, 'meso_df_2_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'meso_df_2_optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    \n",
    "\n",
    "    return test_loss, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, learning_rate, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 10 epochs\"\"\"\n",
    "    lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []\n",
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    train_losses, train_scores = train(log_interval, model, device, train_loader, optimizer, epoch)\n",
    "    epoch_test_loss, epoch_test_score = validation(model, device, optimizer, valid_loader)\n",
    "    # Reduce learning-rate by a factor of 1/10 after every 10 epochs\n",
    "    adjust_learning_rate(optimizer=optimizer, learning_rate=learning_rate, epoch=epoch)\n",
    "    # save results\n",
    "    writer_train.add_scalar('loss',train_losses,epoch+1)\n",
    "    writer_train.add_scalar('score',train_scores,epoch+1)\n",
    "    writer_test.add_scalar('loss',epoch_test_loss,epoch+1)\n",
    "    writer_test.add_scalar('score',epoch_test_score,epoch+1)\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    epoch_train_scores.append(train_scores)\n",
    "    epoch_test_losses.append(epoch_test_loss)\n",
    "    epoch_test_scores.append(epoch_test_score)\n",
    "    #Empty the cache\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
