{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data1\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "class Dataload_3D_CNN(data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        \"Initialization\"\n",
    "        self.transform = transform\n",
    "        #self.frames = frames\n",
    "        self.folders = data_path\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(os.listdir(self.folders))\n",
    "\n",
    "    def read_images(self, data_path, use_transform):\n",
    "        X = []\n",
    "        for i in os.listdir(data_path):\n",
    "            #print(\"file name is \",i)\n",
    "            image = Image.open(os.path.join(data_path,i))\n",
    "            \n",
    "            #print(image.shape)\n",
    "            if use_transform is not None:\n",
    "                image = use_transform(image)\n",
    "                #print(image.size)\n",
    "            image = torch.from_numpy(np.asarray(image))\n",
    "            X.append(image)\n",
    "        #print(X)\n",
    "        #X = np.array(X)\n",
    "        X = torch.stack(X, dim=0)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        # Select sample\n",
    "        #print(\"index passed is \",index)\n",
    "        #print(self.folders)\n",
    "        data_path = os.path.join(self.folders,os.listdir(self.folders)[index])\n",
    "        #data_path = self.folders+ str(index)\n",
    "        #print(\"Data path is \",data_path)\n",
    "        \n",
    "        # Load data\n",
    "        X = self.read_images(data_path, self.transform)                     # (input) spatial images\n",
    "        \n",
    "        y = 1\n",
    "        if 'orig' in data_path:\n",
    "            y = 0\n",
    "        # print(X.shape)\n",
    "        return X, torch.from_numpy(np.array(y)).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_IMG = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    #transforms.ToTensor()\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         #std=[0.229, 0.224, 0.225] )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_path = '/home/chinmay/datatset/train/'\n",
    "train_data = Dataload_3D_CNN(train_path, transform=TRANSFORM_IMG)\n",
    "# for step, (x, y) in enumerate(data):\n",
    "#     print(x.shape)\n",
    "val_path = '/home/chinmay/datatset/val/'\n",
    "val_data = Dataload_3D_CNN(val_path, transform=TRANSFORM_IMG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "log_interval = 10\n",
    "img_x, img_y = 96,96#128,128#256, 256  # resize video 2d frame size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is use_cuda True\n"
     ]
    }
   ],
   "source": [
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "print(\"Is use_cuda\", use_cuda)\n",
    "# Now load the dataset\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "# Load the dataset\n",
    "\n",
    "train_loader = data1.DataLoader(train_data, **params)\n",
    "valid_loader = data1.DataLoader(val_data, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_median(numpy_array = []): #This is expected to take an array of array. So,\n",
    "    #print(\"Input array is {}\".format(numpy_array))\n",
    "    output = []\n",
    "    confidence_scores = []\n",
    "    for array in numpy_array:\n",
    "        counts = np.bincount(array)\n",
    "        output.append(np.argmax(counts))\n",
    "        # Let us compute the confidence of the scores\n",
    "        # since frames are independent, our confidence is purely based on the number\n",
    "        # of frames our model thinks is belonging to a specific category\n",
    "        # the confidence of individual frame prediction is not taken into consideration\n",
    "        # and this portion is debatable....\n",
    "        frame_set_pred = np.sort(counts)[-1]\n",
    "        confidence = frame_set_pred/sum(counts)\n",
    "        confidence_scores.append(confidence)\n",
    "    return torch.from_numpy(np.asarray(output)).type(torch.LongTensor), torch.from_numpy(np.asarray(confidence_scores)).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "writer_train = SummaryWriter('/home/chinmay/training-results/conv3D_refined_f2f/train')\n",
    "writer_test = SummaryWriter('/home/chinmay/training-results/conv3D_refined_f2f/test')\n",
    "save_model_path = \"/home/chinmay/model_weights/conv3D_f2f/\"\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 3D CNN parameters\n",
    "fc_hidden1, fc_hidden2 = 256, 256\n",
    "dropout = 0.0        # dropout probability\n",
    "\n",
    "\n",
    "# Select which frame to begin & end in videos\n",
    "begin_frame, end_frame, skip_frame = 1, 10, 1\n",
    "\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        #X, y = X.to(device), y.to(device)\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        #print(\"The label is \",y)\n",
    "        N_count += X.size(0)\n",
    "        #print(\"The size is \",X.size())\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)  # output size = (batch, number of classes)\n",
    "        #y, _ = find_median(y) #This is necessary as now only single label output for entire frame\n",
    "        y = y.to(device)\n",
    "        #print(y)\n",
    "        #print(y.shape)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "                 \n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "         \n",
    "    return np.mean(losses), np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    model.eval()\n",
    "\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    test_loss = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            #X, y = X.to(device), y.to(device)\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            output = model(X)\n",
    "\n",
    "            #y, _ = find_median(y) #This is necessary as now only single label output for entire frame\n",
    "            y = y.to(device)\n",
    "            loss = F.cross_entropy(output, y)\n",
    "            test_loss.append(loss.item())                 # sum up batch loss\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "            \n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss = np.mean(test_loss)\n",
    "\n",
    "    # to compute accuracy\n",
    "#     all_y = torch.stack(all_y, dim=0)\n",
    "#     all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(model.state_dict(), os.path.join(save_model_path, 'cnn3d_f2f{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'cnn3d_f2f_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "\n",
    "    return test_loss, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [80/1408 (6%)]\tLoss: 0.729291, Accu: 37.50%\n",
      "Train Epoch: 1 [160/1408 (11%)]\tLoss: 0.929031, Accu: 12.50%\n",
      "Train Epoch: 1 [240/1408 (17%)]\tLoss: 0.657839, Accu: 62.50%\n",
      "Train Epoch: 1 [320/1408 (23%)]\tLoss: 0.717681, Accu: 50.00%\n",
      "Train Epoch: 1 [400/1408 (28%)]\tLoss: 0.673437, Accu: 50.00%\n",
      "Train Epoch: 1 [480/1408 (34%)]\tLoss: 0.747937, Accu: 25.00%\n",
      "Train Epoch: 1 [560/1408 (40%)]\tLoss: 0.706535, Accu: 62.50%\n",
      "Train Epoch: 1 [640/1408 (45%)]\tLoss: 0.664084, Accu: 75.00%\n",
      "Train Epoch: 1 [720/1408 (51%)]\tLoss: 0.638441, Accu: 62.50%\n",
      "Train Epoch: 1 [800/1408 (57%)]\tLoss: 0.699842, Accu: 50.00%\n",
      "Train Epoch: 1 [880/1408 (62%)]\tLoss: 0.701974, Accu: 50.00%\n",
      "Train Epoch: 1 [960/1408 (68%)]\tLoss: 0.702490, Accu: 50.00%\n",
      "Train Epoch: 1 [1040/1408 (74%)]\tLoss: 0.748188, Accu: 25.00%\n",
      "Train Epoch: 1 [1120/1408 (80%)]\tLoss: 0.697335, Accu: 62.50%\n",
      "Train Epoch: 1 [1200/1408 (85%)]\tLoss: 0.714039, Accu: 62.50%\n",
      "Train Epoch: 1 [1280/1408 (91%)]\tLoss: 0.686208, Accu: 50.00%\n",
      "Train Epoch: 1 [1360/1408 (97%)]\tLoss: 0.672939, Accu: 62.50%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.6945, Accuracy: 50.67%\n",
      "\n",
      "Epoch 1 model saved!\n",
      "Train Epoch: 2 [80/1408 (6%)]\tLoss: 0.798817, Accu: 25.00%\n",
      "Train Epoch: 2 [160/1408 (11%)]\tLoss: 0.679466, Accu: 62.50%\n",
      "Train Epoch: 2 [240/1408 (17%)]\tLoss: 0.681655, Accu: 50.00%\n",
      "Train Epoch: 2 [320/1408 (23%)]\tLoss: 0.656791, Accu: 75.00%\n",
      "Train Epoch: 2 [400/1408 (28%)]\tLoss: 0.694885, Accu: 37.50%\n",
      "Train Epoch: 2 [480/1408 (34%)]\tLoss: 0.725998, Accu: 50.00%\n",
      "Train Epoch: 2 [560/1408 (40%)]\tLoss: 0.695874, Accu: 37.50%\n",
      "Train Epoch: 2 [640/1408 (45%)]\tLoss: 0.652138, Accu: 62.50%\n",
      "Train Epoch: 2 [720/1408 (51%)]\tLoss: 0.637800, Accu: 50.00%\n",
      "Train Epoch: 2 [800/1408 (57%)]\tLoss: 0.699515, Accu: 50.00%\n",
      "Train Epoch: 2 [880/1408 (62%)]\tLoss: 0.628481, Accu: 75.00%\n",
      "Train Epoch: 2 [960/1408 (68%)]\tLoss: 0.671086, Accu: 75.00%\n",
      "Train Epoch: 2 [1040/1408 (74%)]\tLoss: 0.778356, Accu: 37.50%\n",
      "Train Epoch: 2 [1120/1408 (80%)]\tLoss: 0.626650, Accu: 50.00%\n",
      "Train Epoch: 2 [1200/1408 (85%)]\tLoss: 0.786483, Accu: 50.00%\n",
      "Train Epoch: 2 [1280/1408 (91%)]\tLoss: 0.634121, Accu: 62.50%\n",
      "Train Epoch: 2 [1360/1408 (97%)]\tLoss: 0.663157, Accu: 62.50%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.6950, Accuracy: 52.67%\n",
      "\n",
      "Epoch 2 model saved!\n",
      "Train Epoch: 3 [80/1408 (6%)]\tLoss: 0.709056, Accu: 62.50%\n",
      "Train Epoch: 3 [160/1408 (11%)]\tLoss: 0.672032, Accu: 62.50%\n",
      "Train Epoch: 3 [240/1408 (17%)]\tLoss: 0.580583, Accu: 75.00%\n",
      "Train Epoch: 3 [320/1408 (23%)]\tLoss: 0.533636, Accu: 87.50%\n",
      "Train Epoch: 3 [400/1408 (28%)]\tLoss: 0.652645, Accu: 62.50%\n",
      "Train Epoch: 3 [480/1408 (34%)]\tLoss: 0.492206, Accu: 75.00%\n",
      "Train Epoch: 3 [560/1408 (40%)]\tLoss: 0.516698, Accu: 87.50%\n",
      "Train Epoch: 3 [640/1408 (45%)]\tLoss: 0.703559, Accu: 62.50%\n",
      "Train Epoch: 3 [720/1408 (51%)]\tLoss: 0.666004, Accu: 50.00%\n",
      "Train Epoch: 3 [800/1408 (57%)]\tLoss: 0.590456, Accu: 75.00%\n",
      "Train Epoch: 3 [880/1408 (62%)]\tLoss: 0.639249, Accu: 62.50%\n",
      "Train Epoch: 3 [960/1408 (68%)]\tLoss: 0.628767, Accu: 50.00%\n",
      "Train Epoch: 3 [1040/1408 (74%)]\tLoss: 0.751563, Accu: 50.00%\n",
      "Train Epoch: 3 [1120/1408 (80%)]\tLoss: 0.722213, Accu: 50.00%\n",
      "Train Epoch: 3 [1200/1408 (85%)]\tLoss: 0.919541, Accu: 37.50%\n",
      "Train Epoch: 3 [1280/1408 (91%)]\tLoss: 0.812739, Accu: 50.00%\n",
      "Train Epoch: 3 [1360/1408 (97%)]\tLoss: 0.639767, Accu: 62.50%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.7155, Accuracy: 56.33%\n",
      "\n",
      "Epoch 3 model saved!\n",
      "Train Epoch: 4 [80/1408 (6%)]\tLoss: 0.530384, Accu: 87.50%\n",
      "Train Epoch: 4 [160/1408 (11%)]\tLoss: 0.612401, Accu: 62.50%\n",
      "Train Epoch: 4 [240/1408 (17%)]\tLoss: 0.509287, Accu: 87.50%\n",
      "Train Epoch: 4 [320/1408 (23%)]\tLoss: 0.507500, Accu: 75.00%\n",
      "Train Epoch: 4 [400/1408 (28%)]\tLoss: 0.583065, Accu: 75.00%\n",
      "Train Epoch: 4 [480/1408 (34%)]\tLoss: 0.748049, Accu: 50.00%\n",
      "Train Epoch: 4 [560/1408 (40%)]\tLoss: 0.440979, Accu: 75.00%\n",
      "Train Epoch: 4 [640/1408 (45%)]\tLoss: 0.809380, Accu: 37.50%\n",
      "Train Epoch: 4 [720/1408 (51%)]\tLoss: 0.658928, Accu: 62.50%\n",
      "Train Epoch: 4 [800/1408 (57%)]\tLoss: 0.692655, Accu: 62.50%\n",
      "Train Epoch: 4 [880/1408 (62%)]\tLoss: 0.609558, Accu: 75.00%\n",
      "Train Epoch: 4 [960/1408 (68%)]\tLoss: 0.652236, Accu: 62.50%\n",
      "Train Epoch: 4 [1040/1408 (74%)]\tLoss: 0.562767, Accu: 62.50%\n",
      "Train Epoch: 4 [1120/1408 (80%)]\tLoss: 0.678604, Accu: 50.00%\n",
      "Train Epoch: 4 [1200/1408 (85%)]\tLoss: 0.738001, Accu: 50.00%\n",
      "Train Epoch: 4 [1280/1408 (91%)]\tLoss: 0.571571, Accu: 75.00%\n",
      "Train Epoch: 4 [1360/1408 (97%)]\tLoss: 0.446079, Accu: 87.50%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.6173, Accuracy: 68.33%\n",
      "\n",
      "Epoch 4 model saved!\n",
      "Train Epoch: 5 [80/1408 (6%)]\tLoss: 0.440095, Accu: 87.50%\n",
      "Train Epoch: 5 [160/1408 (11%)]\tLoss: 0.755132, Accu: 50.00%\n",
      "Train Epoch: 5 [240/1408 (17%)]\tLoss: 0.597799, Accu: 87.50%\n",
      "Train Epoch: 5 [320/1408 (23%)]\tLoss: 0.415238, Accu: 87.50%\n",
      "Train Epoch: 5 [400/1408 (28%)]\tLoss: 0.605397, Accu: 62.50%\n",
      "Train Epoch: 5 [480/1408 (34%)]\tLoss: 0.465292, Accu: 87.50%\n",
      "Train Epoch: 5 [560/1408 (40%)]\tLoss: 0.436602, Accu: 62.50%\n",
      "Train Epoch: 5 [640/1408 (45%)]\tLoss: 0.995224, Accu: 50.00%\n",
      "Train Epoch: 5 [720/1408 (51%)]\tLoss: 0.536331, Accu: 75.00%\n",
      "Train Epoch: 5 [800/1408 (57%)]\tLoss: 0.446365, Accu: 75.00%\n",
      "Train Epoch: 5 [880/1408 (62%)]\tLoss: 0.309704, Accu: 100.00%\n",
      "Train Epoch: 5 [960/1408 (68%)]\tLoss: 0.635173, Accu: 62.50%\n",
      "Train Epoch: 5 [1040/1408 (74%)]\tLoss: 0.755408, Accu: 50.00%\n",
      "Train Epoch: 5 [1120/1408 (80%)]\tLoss: 0.455885, Accu: 75.00%\n",
      "Train Epoch: 5 [1200/1408 (85%)]\tLoss: 0.695167, Accu: 50.00%\n",
      "Train Epoch: 5 [1280/1408 (91%)]\tLoss: 0.504655, Accu: 75.00%\n",
      "Train Epoch: 5 [1360/1408 (97%)]\tLoss: 0.702986, Accu: 50.00%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.5703, Accuracy: 72.67%\n",
      "\n",
      "Epoch 5 model saved!\n",
      "Train Epoch: 6 [80/1408 (6%)]\tLoss: 0.336687, Accu: 87.50%\n",
      "Train Epoch: 6 [160/1408 (11%)]\tLoss: 0.306526, Accu: 87.50%\n",
      "Train Epoch: 6 [240/1408 (17%)]\tLoss: 0.310773, Accu: 87.50%\n",
      "Train Epoch: 6 [320/1408 (23%)]\tLoss: 0.699993, Accu: 75.00%\n",
      "Train Epoch: 6 [400/1408 (28%)]\tLoss: 0.327599, Accu: 87.50%\n",
      "Train Epoch: 6 [480/1408 (34%)]\tLoss: 0.396562, Accu: 87.50%\n",
      "Train Epoch: 6 [560/1408 (40%)]\tLoss: 0.715775, Accu: 50.00%\n",
      "Train Epoch: 6 [640/1408 (45%)]\tLoss: 0.654268, Accu: 62.50%\n",
      "Train Epoch: 6 [720/1408 (51%)]\tLoss: 0.490150, Accu: 62.50%\n",
      "Train Epoch: 6 [800/1408 (57%)]\tLoss: 0.398962, Accu: 87.50%\n",
      "Train Epoch: 6 [880/1408 (62%)]\tLoss: 0.454750, Accu: 87.50%\n",
      "Train Epoch: 6 [960/1408 (68%)]\tLoss: 0.332158, Accu: 87.50%\n",
      "Train Epoch: 6 [1040/1408 (74%)]\tLoss: 0.673156, Accu: 50.00%\n",
      "Train Epoch: 6 [1120/1408 (80%)]\tLoss: 0.462244, Accu: 75.00%\n",
      "Train Epoch: 6 [1200/1408 (85%)]\tLoss: 0.397258, Accu: 87.50%\n",
      "Train Epoch: 6 [1280/1408 (91%)]\tLoss: 0.310558, Accu: 100.00%\n",
      "Train Epoch: 6 [1360/1408 (97%)]\tLoss: 0.482685, Accu: 87.50%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.5591, Accuracy: 73.00%\n",
      "\n",
      "Epoch 6 model saved!\n",
      "Train Epoch: 7 [80/1408 (6%)]\tLoss: 0.393022, Accu: 87.50%\n",
      "Train Epoch: 7 [160/1408 (11%)]\tLoss: 0.776501, Accu: 50.00%\n",
      "Train Epoch: 7 [240/1408 (17%)]\tLoss: 0.551825, Accu: 75.00%\n",
      "Train Epoch: 7 [320/1408 (23%)]\tLoss: 0.452096, Accu: 75.00%\n",
      "Train Epoch: 7 [400/1408 (28%)]\tLoss: 0.462452, Accu: 75.00%\n",
      "Train Epoch: 7 [480/1408 (34%)]\tLoss: 0.513512, Accu: 62.50%\n",
      "Train Epoch: 7 [560/1408 (40%)]\tLoss: 0.359860, Accu: 87.50%\n",
      "Train Epoch: 7 [640/1408 (45%)]\tLoss: 0.643830, Accu: 62.50%\n",
      "Train Epoch: 7 [720/1408 (51%)]\tLoss: 0.240350, Accu: 100.00%\n",
      "Train Epoch: 7 [800/1408 (57%)]\tLoss: 0.513417, Accu: 87.50%\n",
      "Train Epoch: 7 [880/1408 (62%)]\tLoss: 0.498184, Accu: 75.00%\n",
      "Train Epoch: 7 [960/1408 (68%)]\tLoss: 0.130329, Accu: 100.00%\n",
      "Train Epoch: 7 [1040/1408 (74%)]\tLoss: 0.921584, Accu: 50.00%\n",
      "Train Epoch: 7 [1120/1408 (80%)]\tLoss: 1.077390, Accu: 37.50%\n",
      "Train Epoch: 7 [1200/1408 (85%)]\tLoss: 0.616395, Accu: 50.00%\n",
      "Train Epoch: 7 [1280/1408 (91%)]\tLoss: 0.422364, Accu: 75.00%\n",
      "Train Epoch: 7 [1360/1408 (97%)]\tLoss: 0.452796, Accu: 75.00%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.5720, Accuracy: 69.00%\n",
      "\n",
      "Epoch 7 model saved!\n",
      "Train Epoch: 8 [80/1408 (6%)]\tLoss: 0.389542, Accu: 75.00%\n",
      "Train Epoch: 8 [160/1408 (11%)]\tLoss: 0.642712, Accu: 75.00%\n",
      "Train Epoch: 8 [240/1408 (17%)]\tLoss: 0.345818, Accu: 87.50%\n",
      "Train Epoch: 8 [320/1408 (23%)]\tLoss: 0.381275, Accu: 87.50%\n",
      "Train Epoch: 8 [400/1408 (28%)]\tLoss: 0.619479, Accu: 75.00%\n",
      "Train Epoch: 8 [480/1408 (34%)]\tLoss: 0.289447, Accu: 75.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [560/1408 (40%)]\tLoss: 0.608178, Accu: 62.50%\n",
      "Train Epoch: 8 [640/1408 (45%)]\tLoss: 0.365566, Accu: 87.50%\n",
      "Train Epoch: 8 [720/1408 (51%)]\tLoss: 0.326768, Accu: 75.00%\n",
      "Train Epoch: 8 [800/1408 (57%)]\tLoss: 0.719186, Accu: 62.50%\n",
      "Train Epoch: 8 [880/1408 (62%)]\tLoss: 0.951225, Accu: 37.50%\n",
      "Train Epoch: 8 [960/1408 (68%)]\tLoss: 0.267050, Accu: 87.50%\n",
      "Train Epoch: 8 [1040/1408 (74%)]\tLoss: 0.946567, Accu: 37.50%\n",
      "Train Epoch: 8 [1120/1408 (80%)]\tLoss: 0.480469, Accu: 75.00%\n",
      "Train Epoch: 8 [1200/1408 (85%)]\tLoss: 0.406097, Accu: 62.50%\n",
      "Train Epoch: 8 [1280/1408 (91%)]\tLoss: 0.592011, Accu: 75.00%\n",
      "Train Epoch: 8 [1360/1408 (97%)]\tLoss: 0.654362, Accu: 75.00%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.5012, Accuracy: 74.33%\n",
      "\n",
      "Epoch 8 model saved!\n",
      "Train Epoch: 9 [80/1408 (6%)]\tLoss: 0.483445, Accu: 87.50%\n",
      "Train Epoch: 9 [160/1408 (11%)]\tLoss: 0.927527, Accu: 37.50%\n",
      "Train Epoch: 9 [240/1408 (17%)]\tLoss: 0.433038, Accu: 75.00%\n",
      "Train Epoch: 9 [320/1408 (23%)]\tLoss: 0.611623, Accu: 62.50%\n",
      "Train Epoch: 9 [400/1408 (28%)]\tLoss: 0.414912, Accu: 87.50%\n",
      "Train Epoch: 9 [480/1408 (34%)]\tLoss: 0.278261, Accu: 87.50%\n",
      "Train Epoch: 9 [560/1408 (40%)]\tLoss: 0.116153, Accu: 100.00%\n",
      "Train Epoch: 9 [640/1408 (45%)]\tLoss: 0.517879, Accu: 75.00%\n",
      "Train Epoch: 9 [720/1408 (51%)]\tLoss: 0.414235, Accu: 87.50%\n",
      "Train Epoch: 9 [800/1408 (57%)]\tLoss: 0.356257, Accu: 87.50%\n",
      "Train Epoch: 9 [880/1408 (62%)]\tLoss: 0.331913, Accu: 87.50%\n",
      "Train Epoch: 9 [960/1408 (68%)]\tLoss: 0.379985, Accu: 87.50%\n",
      "Train Epoch: 9 [1040/1408 (74%)]\tLoss: 0.414869, Accu: 87.50%\n",
      "Train Epoch: 9 [1120/1408 (80%)]\tLoss: 0.350832, Accu: 87.50%\n",
      "Train Epoch: 9 [1200/1408 (85%)]\tLoss: 0.345738, Accu: 75.00%\n",
      "Train Epoch: 9 [1280/1408 (91%)]\tLoss: 0.277612, Accu: 75.00%\n",
      "Train Epoch: 9 [1360/1408 (97%)]\tLoss: 0.562606, Accu: 75.00%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.5562, Accuracy: 69.67%\n",
      "\n",
      "Epoch 9 model saved!\n",
      "Train Epoch: 10 [80/1408 (6%)]\tLoss: 0.140153, Accu: 100.00%\n",
      "Train Epoch: 10 [160/1408 (11%)]\tLoss: 0.574937, Accu: 87.50%\n",
      "Train Epoch: 10 [240/1408 (17%)]\tLoss: 0.323003, Accu: 100.00%\n",
      "Train Epoch: 10 [320/1408 (23%)]\tLoss: 0.381862, Accu: 75.00%\n",
      "Train Epoch: 10 [400/1408 (28%)]\tLoss: 0.437251, Accu: 75.00%\n",
      "Train Epoch: 10 [480/1408 (34%)]\tLoss: 0.174509, Accu: 100.00%\n",
      "Train Epoch: 10 [560/1408 (40%)]\tLoss: 0.981121, Accu: 62.50%\n",
      "Train Epoch: 10 [640/1408 (45%)]\tLoss: 0.616556, Accu: 62.50%\n",
      "Train Epoch: 10 [720/1408 (51%)]\tLoss: 0.354033, Accu: 100.00%\n",
      "Train Epoch: 10 [800/1408 (57%)]\tLoss: 0.418086, Accu: 75.00%\n",
      "Train Epoch: 10 [880/1408 (62%)]\tLoss: 0.373848, Accu: 75.00%\n",
      "Train Epoch: 10 [960/1408 (68%)]\tLoss: 0.644944, Accu: 62.50%\n",
      "Train Epoch: 10 [1040/1408 (74%)]\tLoss: 0.566581, Accu: 75.00%\n",
      "Train Epoch: 10 [1120/1408 (80%)]\tLoss: 0.600944, Accu: 75.00%\n",
      "Train Epoch: 10 [1200/1408 (85%)]\tLoss: 0.432594, Accu: 87.50%\n",
      "Train Epoch: 10 [1280/1408 (91%)]\tLoss: 0.210472, Accu: 100.00%\n",
      "Train Epoch: 10 [1360/1408 (97%)]\tLoss: 0.331859, Accu: 87.50%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.5047, Accuracy: 76.33%\n",
      "\n",
      "Epoch 10 model saved!\n",
      "Train Epoch: 11 [80/1408 (6%)]\tLoss: 0.222022, Accu: 87.50%\n",
      "Train Epoch: 11 [160/1408 (11%)]\tLoss: 0.275154, Accu: 87.50%\n",
      "Train Epoch: 11 [240/1408 (17%)]\tLoss: 0.307556, Accu: 75.00%\n",
      "Train Epoch: 11 [320/1408 (23%)]\tLoss: 0.195236, Accu: 100.00%\n",
      "Train Epoch: 11 [400/1408 (28%)]\tLoss: 0.356400, Accu: 87.50%\n",
      "Train Epoch: 11 [480/1408 (34%)]\tLoss: 0.224191, Accu: 100.00%\n",
      "Train Epoch: 11 [560/1408 (40%)]\tLoss: 0.428568, Accu: 87.50%\n",
      "Train Epoch: 11 [640/1408 (45%)]\tLoss: 0.643482, Accu: 62.50%\n",
      "Train Epoch: 11 [720/1408 (51%)]\tLoss: 0.429945, Accu: 75.00%\n",
      "Train Epoch: 11 [800/1408 (57%)]\tLoss: 0.346396, Accu: 87.50%\n",
      "Train Epoch: 11 [880/1408 (62%)]\tLoss: 0.435430, Accu: 62.50%\n",
      "Train Epoch: 11 [960/1408 (68%)]\tLoss: 0.253498, Accu: 87.50%\n",
      "Train Epoch: 11 [1040/1408 (74%)]\tLoss: 0.340475, Accu: 87.50%\n",
      "Train Epoch: 11 [1120/1408 (80%)]\tLoss: 0.346383, Accu: 87.50%\n",
      "Train Epoch: 11 [1200/1408 (85%)]\tLoss: 0.631751, Accu: 62.50%\n",
      "Train Epoch: 11 [1280/1408 (91%)]\tLoss: 0.361012, Accu: 75.00%\n",
      "Train Epoch: 11 [1360/1408 (97%)]\tLoss: 0.855143, Accu: 37.50%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.6283, Accuracy: 70.00%\n",
      "\n",
      "Epoch 11 model saved!\n",
      "Train Epoch: 12 [80/1408 (6%)]\tLoss: 0.582328, Accu: 75.00%\n",
      "Train Epoch: 12 [160/1408 (11%)]\tLoss: 0.429610, Accu: 87.50%\n",
      "Train Epoch: 12 [240/1408 (17%)]\tLoss: 0.256880, Accu: 87.50%\n",
      "Train Epoch: 12 [320/1408 (23%)]\tLoss: 0.375240, Accu: 75.00%\n",
      "Train Epoch: 12 [400/1408 (28%)]\tLoss: 0.539464, Accu: 75.00%\n",
      "Train Epoch: 12 [480/1408 (34%)]\tLoss: 0.298734, Accu: 87.50%\n",
      "Train Epoch: 12 [560/1408 (40%)]\tLoss: 0.358489, Accu: 87.50%\n",
      "Train Epoch: 12 [640/1408 (45%)]\tLoss: 0.220255, Accu: 87.50%\n",
      "Train Epoch: 12 [720/1408 (51%)]\tLoss: 0.152759, Accu: 100.00%\n",
      "Train Epoch: 12 [800/1408 (57%)]\tLoss: 0.497511, Accu: 75.00%\n",
      "Train Epoch: 12 [880/1408 (62%)]\tLoss: 0.133196, Accu: 87.50%\n",
      "Train Epoch: 12 [960/1408 (68%)]\tLoss: 0.553089, Accu: 75.00%\n",
      "Train Epoch: 12 [1040/1408 (74%)]\tLoss: 0.464757, Accu: 62.50%\n",
      "Train Epoch: 12 [1120/1408 (80%)]\tLoss: 0.204370, Accu: 87.50%\n",
      "Train Epoch: 12 [1200/1408 (85%)]\tLoss: 0.206004, Accu: 100.00%\n",
      "Train Epoch: 12 [1280/1408 (91%)]\tLoss: 0.263818, Accu: 87.50%\n",
      "Train Epoch: 12 [1360/1408 (97%)]\tLoss: 0.263324, Accu: 87.50%\n",
      "\n",
      "Test set (300 samples): Average loss: 0.5657, Accuracy: 73.67%\n",
      "\n",
      "Epoch 12 model saved!\n",
      "Train Epoch: 13 [80/1408 (6%)]\tLoss: 0.258639, Accu: 87.50%\n",
      "Train Epoch: 13 [160/1408 (11%)]\tLoss: 0.324767, Accu: 75.00%\n",
      "Train Epoch: 13 [240/1408 (17%)]\tLoss: 0.156554, Accu: 100.00%\n",
      "Train Epoch: 13 [320/1408 (23%)]\tLoss: 0.245535, Accu: 87.50%\n",
      "Train Epoch: 13 [400/1408 (28%)]\tLoss: 0.359113, Accu: 75.00%\n",
      "Train Epoch: 13 [480/1408 (34%)]\tLoss: 0.113489, Accu: 100.00%\n",
      "Train Epoch: 13 [560/1408 (40%)]\tLoss: 0.298318, Accu: 87.50%\n",
      "Train Epoch: 13 [640/1408 (45%)]\tLoss: 0.155442, Accu: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-100:\n",
      "Process Process-99:\n",
      "Process Process-98:\n",
      "Process Process-97:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/chinmay/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-79a86eb9e6cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# train, test model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mepoch_test_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_test_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5d6f3e47cd46>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(log_interval, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# distribute data to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#X, y = X.to(device), y.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m#print(\"The label is \",y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mN_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create model\n",
    "#cnn3d = CNN3D(t_dim=10, img_x=img_x, img_y=img_y,\n",
    "#              drop_p=dropout, fc_hidden1=fc_hidden1,  fc_hidden2=fc_hidden2, num_classes=2)\n",
    "\n",
    "from Res3D import C3D\n",
    "cnn3d = C3D(img_dim=256, frames=10, dropout=0.4)\n",
    "\n",
    "cnn3d.cuda()\n",
    "# Parallelize model to multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    cnn3d = nn.DataParallel(cnn3d)\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn3d.parameters(), lr=learning_rate)   # optimize all cnn parameters\n",
    "\n",
    "\n",
    "# record training process\n",
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []\n",
    "\n",
    "\n",
    "# start training\n",
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    train_losses, train_scores = train(log_interval, cnn3d, device, train_loader, optimizer, epoch)\n",
    "    epoch_test_loss, epoch_test_score = validation(cnn3d, device, optimizer, valid_loader)\n",
    "\n",
    "    # save all train test results\n",
    "    # save results\n",
    "    writer_train.add_scalar('loss',train_losses,epoch+1)\n",
    "    writer_train.add_scalar('score',train_scores,epoch+1)\n",
    "    writer_test.add_scalar('loss',epoch_test_loss,epoch+1)\n",
    "    writer_test.add_scalar('score',epoch_test_score,epoch+1)\n",
    "    \n",
    "    \n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
